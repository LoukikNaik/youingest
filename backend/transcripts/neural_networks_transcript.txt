Transcript for: But what is a neural network? | Deep learning chapter 1
======================================================================

[00:00:04 - 00:00:05]
This is a 3.

[00:00:06 - 00:00:10]
It's sloppily written and rendered at an extremely low resolution of 28x28 pixels,

[00:00:10 - 00:00:13]
but your brain has no trouble recognizing it as a 3.

[00:00:14 - 00:00:16]
And I want you to take a moment to appreciate how

[00:00:16 - 00:00:18]
crazy it is that brains can do this so effortlessly.

[00:00:19 - 00:00:22]
I mean, this, this and this are also recognizable as 3s,

[00:00:22 - 00:00:27]
even though the specific values of each pixel is very different from one

[00:00:27 - 00:00:28]
image to the next.

[00:00:28 - 00:00:32]
The particular light-sensitive cells in your eye that are firing when you

[00:00:32 - 00:00:36]
see this 3 are very different from the ones firing when you see this 3.

[00:00:37 - 00:00:42]
But something in that crazy-smart visual cortex of yours resolves these as representing

[00:00:42 - 00:00:47]
the same idea, while at the same time recognizing other images as their own distinct

[00:00:47 - 00:00:48]
ideas.

[00:00:49 - 00:00:54]
But if I told you, hey, sit down and write for me a program that takes in a grid of

[00:00:54 - 00:00:59]
28x28 pixels like this and outputs a single number between 0 and 10,

[00:00:59 - 00:01:04]
telling you what it thinks the digit is, well the task goes from comically trivial to

[00:01:04 - 00:01:06]
dauntingly difficult.

[00:01:07 - 00:01:10]
Unless you've been living under a rock, I think I hardly need to motivate the relevance

[00:01:10 - 00:01:14]
and importance of machine learning and neural networks to the present and to the future.

[00:01:15 - 00:01:18]
But what I want to do here is show you what a neural network actually is,

[00:01:18 - 00:01:22]
assuming no background, and to help visualize what it's doing,

[00:01:22 - 00:01:24]
not as a buzzword but as a piece of math.

[00:01:25 - 00:01:28]
My hope is that you come away feeling like the structure itself is motivated,

[00:01:28 - 00:01:31]
and to feel like you know what it means when you read,

[00:01:31 - 00:01:34]
or you hear about a neural network quote-unquote learning.

[00:01:35 - 00:01:38]
This video is just going to be devoted to the structure component of that,

[00:01:38 - 00:01:40]
and the following one is going to tackle learning.

[00:01:40 - 00:01:43]
What we're going to do is put together a neural

[00:01:43 - 00:01:46]
network that can learn to recognize handwritten digits.

[00:01:49 - 00:01:52]
This is a somewhat classic example for introducing the topic,

[00:01:52 - 00:01:54]
and I'm happy to stick with the status quo here,

[00:01:54 - 00:01:57]
because at the end of the two videos I want to point you to a couple good

[00:01:57 - 00:02:00]
resources where you can learn more, and where you can download the code that

[00:02:00 - 00:02:03]
does this and play with it on your own computer.

[00:02:05 - 00:02:07]
There are many many variants of neural networks,

[00:02:07 - 00:02:12]
and in recent years there's been sort of a boom in research towards these variants,

[00:02:12 - 00:02:16]
but in these two introductory videos you and I are just going to look at the simplest

[00:02:16 - 00:02:19]
plain vanilla form with no added frills.

[00:02:19 - 00:02:23]
This is kind of a necessary prerequisite for understanding any of the more powerful

[00:02:23 - 00:02:28]
modern variants, and trust me it still has plenty of complexity for us to wrap our minds

[00:02:29 - 00:02:33]
But even in this simplest form it can learn to recognize handwritten digits,

[00:02:33 - 00:02:36]
which is a pretty cool thing for a computer to be able to do.

[00:02:37 - 00:02:39]
And at the same time you'll see how it does fall

[00:02:39 - 00:02:42]
short of a couple hopes that we might have for it.

[00:02:43 - 00:02:48]
As the name suggests neural networks are inspired by the brain, but let's break that down.

[00:02:48 - 00:02:51]
What are the neurons, and in what sense are they linked together?

[00:02:52 - 00:02:58]
Right now when I say neuron all I want you to think about is a thing that holds a number,

[00:02:58 - 00:03:00]
specifically a number between 0 and 1.

[00:03:00 - 00:03:02]
It's really not more than that.

[00:03:03 - 00:03:08]
For example the network starts with a bunch of neurons corresponding to

[00:03:08 - 00:03:14]
each of the 28x28 pixels of the input image, which is 784 neurons in total.

[00:03:14 - 00:03:19]
Each one of these holds a number that represents the grayscale value of the

[00:03:19 - 00:03:24]
corresponding pixel, ranging from 0 for black pixels up to 1 for white pixels.

[00:03:25 - 00:03:28]
This number inside the neuron is called its activation,

[00:03:28 - 00:03:32]
and the image you might have in mind here is that each neuron is lit up when its

[00:03:32 - 00:03:34]
activation is a high number.

[00:03:36 - 00:03:41]
So all of these 784 neurons make up the first layer of our network.

[00:03:46 - 00:03:49]
Now jumping over to the last layer, this has 10 neurons,

[00:03:49 - 00:03:51]
each representing one of the digits.

[00:03:52 - 00:03:56]
The activation in these neurons, again some number that's between 0 and 1,

[00:03:56 - 00:04:02]
represents how much the system thinks that a given image corresponds with a given digit.

[00:04:03 - 00:04:06]
There's also a couple layers in between called the hidden layers,

[00:04:06 - 00:04:09]
which for the time being should just be a giant question mark for

[00:04:09 - 00:04:13]
how on earth this process of recognizing digits is going to be handled.

[00:04:14 - 00:04:17]
In this network I chose two hidden layers, each one with 16 neurons,

[00:04:17 - 00:04:20]
and admittedly that's kind of an arbitrary choice.

[00:04:21 - 00:04:24]
To be honest I chose two layers based on how I want to motivate the structure in

[00:04:24 - 00:04:28]
just a moment, and 16, well that was just a nice number to fit on the screen.

[00:04:28 - 00:04:32]
In practice there is a lot of room for experiment with a specific structure here.

[00:04:33 - 00:04:35]
The way the network operates, activations in one

[00:04:35 - 00:04:38]
layer determine the activations of the next layer.

[00:04:39 - 00:04:43]
And of course the heart of the network as an information processing mechanism comes down

[00:04:43 - 00:04:48]
to exactly how those activations from one layer bring about activations in the next

[00:04:49 - 00:04:53]
It's meant to be loosely analogous to how in biological networks of neurons,

[00:04:53 - 00:04:57]
some groups of neurons firing cause certain others to fire.

[00:04:58 - 00:05:01]
Now the network I'm showing here has already been trained to recognize digits,

[00:05:01 - 00:05:03]
and let me show you what I mean by that.

[00:05:03 - 00:05:08]
It means if you feed in an image, lighting up all 784 neurons of the input layer

[00:05:08 - 00:05:11]
according to the brightness of each pixel in the image,

[00:05:11 - 00:05:16]
that pattern of activations causes some very specific pattern in the next layer

[00:05:16 - 00:05:18]
which causes some pattern in the one after it,

[00:05:18 - 00:05:22]
which finally gives some pattern in the output layer.

[00:05:22 - 00:05:26]
And the brightest neuron of that output layer is the network's choice,

[00:05:26 - 00:05:29]
so to speak, for what digit this image represents.

[00:05:32 - 00:05:36]
And before jumping into the math for how one layer influences the next,

[00:05:36 - 00:05:40]
or how training works, let's just talk about why it's even reasonable

[00:05:40 - 00:05:43]
to expect a layered structure like this to behave intelligently.

[00:05:44 - 00:05:45]
What are we expecting here?

[00:05:45 - 00:05:47]
What is the best hope for what those middle layers might be doing?

[00:05:48 - 00:05:53]
Well, when you or I recognize digits, we piece together various components.

[00:05:54 - 00:05:56]
A 9 has a loop up top and a line on the right.

[00:05:57 - 00:06:01]
An 8 also has a loop up top, but it's paired with another loop down low.

[00:06:01 - 00:06:06]
A 4 basically breaks down into three specific lines, and things like that.

[00:06:07 - 00:06:11]
Now in a perfect world, we might hope that each neuron in the second

[00:06:11 - 00:06:14]
to last layer corresponds with one of these subcomponents,

[00:06:14 - 00:06:18]
that anytime you feed in an image with, say, a loop up top,

[00:06:18 - 00:06:22]
like a 9 or an 8, there's some specific neuron whose activation is

[00:06:22 - 00:06:23]
going to be close to 1.

[00:06:24 - 00:06:26]
And I don't mean this specific loop of pixels,

[00:06:26 - 00:06:31]
the hope would be that any generally loopy pattern towards the top sets off this neuron.

[00:06:32 - 00:06:36]
That way, going from the third layer to the last one just requires

[00:06:36 - 00:06:40]
learning which combination of subcomponents corresponds to which digits.

[00:06:41 - 00:06:43]
Of course, that just kicks the problem down the road,

[00:06:43 - 00:06:45]
because how would you recognize these subcomponents,

[00:06:45 - 00:06:47]
or even learn what the right subcomponents should be?

[00:06:48 - 00:06:51]
And I still haven't even talked about how one layer influences the next,

[00:06:51 - 00:06:53]
but run with me on this one for a moment.

[00:06:53 - 00:06:56]
Recognizing a loop can also break down into subproblems.

[00:06:57 - 00:06:59]
One reasonable way to do this would be to first

[00:06:59 - 00:07:02]
recognize the various little edges that make it up.

[00:07:03 - 00:07:08]
Similarly, a long line, like the kind you might see in the digits 1 or 4 or 7,

[00:07:08 - 00:07:13]
is really just a long edge, or maybe you think of it as a certain pattern of several

[00:07:13 - 00:07:14]
smaller edges.

[00:07:15 - 00:07:18]
So maybe our hope is that each neuron in the second layer of

[00:07:18 - 00:07:22]
the network corresponds with the various relevant little edges.

[00:07:23 - 00:07:27]
Maybe when an image like this one comes in, it lights up all of the

[00:07:27 - 00:07:31]
neurons associated with around 8 to 10 specific little edges,

[00:07:31 - 00:07:35]
which in turn lights up the neurons associated with the upper loop

[00:07:35 - 00:07:39]
and a long vertical line, and those light up the neuron associated with a 9.

[00:07:40 - 00:07:44]
Whether or not this is what our final network actually does is another question,

[00:07:44 - 00:07:47]
one that I'll come back to once we see how to train the network,

[00:07:47 - 00:07:51]
but this is a hope that we might have, a sort of goal with the layered structure

[00:07:51 - 00:07:52]
like this.

[00:07:53 - 00:07:56]
Moreover, you can imagine how being able to detect edges and patterns

[00:07:56 - 00:08:00]
like this would be really useful for other image recognition tasks.

[00:08:00 - 00:08:04]
And even beyond image recognition, there are all sorts of intelligent

[00:08:04 - 00:08:07]
things you might want to do that break down into layers of abstraction.

[00:08:08 - 00:08:12]
Parsing speech, for example, involves taking raw audio and picking out distinct sounds,

[00:08:12 - 00:08:16]
which combine to make certain syllables, which combine to form words,

[00:08:16 - 00:08:20]
which combine to make up phrases and more abstract thoughts, etc.

[00:08:21 - 00:08:23]
But getting back to how any of this actually works,

[00:08:23 - 00:08:27]
picture yourself right now designing how exactly the activations in one layer might

[00:08:27 - 00:08:29]
determine the activations in the next.

[00:08:30 - 00:08:35]
The goal is to have some mechanism that could conceivably combine pixels into edges,

[00:08:35 - 00:08:38]
or edges into patterns, or patterns into digits.

[00:08:39 - 00:08:43]
And to zoom in on one very specific example, let's say the hope

[00:08:43 - 00:08:46]
is for one particular neuron in the second layer to pick up

[00:08:46 - 00:08:50]
on whether or not the image has an edge in this region here.

[00:08:51 - 00:08:55]
The question at hand is what parameters should the network have?

[00:08:55 - 00:08:59]
What dials and knobs should you be able to tweak so that it's expressive

[00:08:59 - 00:09:03]
enough to potentially capture this pattern, or any other pixel pattern,

[00:09:03 - 00:09:07]
or the pattern that several edges can make a loop, and other such things?

[00:09:08 - 00:09:11]
Well, what we'll do is assign a weight to each one of the

[00:09:11 - 00:09:15]
connections between our neuron and the neurons from the first layer.

[00:09:16 - 00:09:17]
These weights are just numbers.

[00:09:18 - 00:09:21]
Then take all of those activations from the first layer

[00:09:21 - 00:09:25]
and compute their weighted sum according to these weights.

[00:09:27 - 00:09:31]
I find it helpful to think of these weights as being organized into a

[00:09:31 - 00:09:34]
little grid of their own, and I'm going to use green pixels to indicate

[00:09:34 - 00:09:37]
positive weights, and red pixels to indicate negative weights,

[00:09:37 - 00:09:41]
where the brightness of that pixel is some loose depiction of the weight's value.

[00:09:42 - 00:09:46]
Now if we made the weights associated with almost all of the pixels zero

[00:09:46 - 00:09:50]
except for some positive weights in this region that we care about,

[00:09:50 - 00:09:53]
then taking the weighted sum of all the pixel values really just amounts

[00:09:53 - 00:09:57]
to adding up the values of the pixel just in the region that we care about.

[00:09:59 - 00:10:02]
And if you really wanted to pick up on whether there's an edge here,

[00:10:02 - 00:10:06]
what you might do is have some negative weights associated with the surrounding pixels.

[00:10:07 - 00:10:10]
Then the sum is largest when those middle pixels

[00:10:10 - 00:10:12]
are bright but the surrounding pixels are darker.

[00:10:14 - 00:10:18]
When you compute a weighted sum like this, you might come out with any number,

[00:10:18 - 00:10:23]
but for this network what we want is for activations to be some value between 0 and 1.

[00:10:24 - 00:10:28]
So a common thing to do is to pump this weighted sum into some function

[00:10:28 - 00:10:32]
that squishes the real number line into the range between 0 and 1.

[00:10:32 - 00:10:35]
And a common function that does this is called the sigmoid function,

[00:10:35 - 00:10:37]
also known as a logistic curve.

[00:10:38 - 00:10:43]
Basically very negative inputs end up close to 0, positive inputs end up close to 1,

[00:10:43 - 00:10:46]
and it just steadily increases around the input 0.

[00:10:49 - 00:10:52]
So the activation of the neuron here is basically a

[00:10:52 - 00:10:56]
measure of how positive the relevant weighted sum is.

[00:10:57 - 00:10:59]
But maybe it's not that you want the neuron to

[00:10:59 - 00:11:01]
light up when the weighted sum is bigger than 0.

[00:11:02 - 00:11:06]
Maybe you only want it to be active when the sum is bigger than say 10.

[00:11:06 - 00:11:10]
That is, you want some bias for it to be inactive.

[00:11:11 - 00:11:15]
What we'll do then is just add in some other number like negative 10 to this

[00:11:15 - 00:11:19]
weighted sum before plugging it through the sigmoid squishification function.

[00:11:20 - 00:11:22]
That additional number is called the bias.

[00:11:23 - 00:11:27]
So the weights tell you what pixel pattern this neuron in the second

[00:11:27 - 00:11:31]
layer is picking up on, and the bias tells you how high the weighted

[00:11:31 - 00:11:35]
sum needs to be before the neuron starts getting meaningfully active.

[00:11:36 - 00:11:37]
And that is just one neuron.

[00:11:38 - 00:11:42]
Every other neuron in this layer is going to be connected to

[00:11:42 - 00:11:46]
all 784 pixel neurons from the first layer, and each one of

[00:11:46 - 00:11:50]
those 784 connections has its own weight associated with it.

[00:11:51 - 00:11:54]
Also, each one has some bias, some other number that you add

[00:11:54 - 00:11:57]
on to the weighted sum before squishing it with the sigmoid.

[00:11:58 - 00:11:59]
And that's a lot to think about!

[00:11:59 - 00:12:06]
With this hidden layer of 16 neurons, that's a total of 784 times 16 weights,

[00:12:06 - 00:12:07]
along with 16 biases.

[00:12:08 - 00:12:11]
And all of that is just the connections from the first layer to the second.

[00:12:12 - 00:12:14]
The connections between the other layers also have

[00:12:14 - 00:12:17]
a bunch of weights and biases associated with them.

[00:12:18 - 00:12:23]
All said and done, this network has almost exactly 13,000 total weights and biases.

[00:12:23 - 00:12:27]
13,000 knobs and dials that can be tweaked and turned

[00:12:27 - 00:12:29]
to make this network behave in different ways.

[00:12:31 - 00:12:34]
So when we talk about learning, what that's referring to is

[00:12:34 - 00:12:37]
getting the computer to find a valid setting for all of these

[00:12:37 - 00:12:41]
many many numbers so that it'll actually solve the problem at hand.

[00:12:42 - 00:12:47]
One thought experiment that is at once fun and kind of horrifying is to imagine sitting

[00:12:47 - 00:12:50]
down and setting all of these weights and biases by hand,

[00:12:50 - 00:12:54]
purposefully tweaking the numbers so that the second layer picks up on edges,

[00:12:54 - 00:12:56]
the third layer picks up on patterns, etc.

[00:12:56 - 00:13:01]
I personally find this satisfying rather than just treating the network as a total black

[00:13:01 - 00:13:04]
box, because when the network doesn't perform the way you anticipate,

[00:13:04 - 00:13:09]
if you've built up a little bit of a relationship with what those weights and biases

[00:13:09 - 00:13:13]
actually mean, you have a starting place for experimenting with how to change the

[00:13:13 - 00:13:14]
structure to improve.

[00:13:14 - 00:13:18]
Or when the network does work but not for the reasons you might expect,

[00:13:18 - 00:13:22]
digging into what the weights and biases are doing is a good way to challenge

[00:13:22 - 00:13:25]
your assumptions and really expose the full space of possible solutions.

[00:13:26 - 00:13:29]
By the way, the actual function here is a little cumbersome to write down,

[00:13:29 - 00:13:30]
don't you think?

[00:13:32 - 00:13:37]
So let me show you a more notationally compact way that these connections are represented.

[00:13:37 - 00:13:40]
This is how you'd see it if you choose to read up more about neural networks.

[00:13:40 - 00:13:48]
Organize all of the activations from one layer into a column as a vector.

[00:13:48 - 00:13:50]
Then organize all of the weights as a matrix, where each row of that matrix corresponds

[00:13:50 - 00:13:58]
to the connections between one layer and a particular neuron in the next layer.

[00:13:58 - 00:14:02]
What that means is that taking the weighted sum of the activations in

[00:14:02 - 00:14:05]
the first layer according to these weights corresponds to one of the

[00:14:05 - 00:14:09]
terms in the matrix vector product of everything we have on the left here.

[00:14:14 - 00:14:17]
By the way, so much of machine learning just comes down to having a good

[00:14:17 - 00:14:21]
grasp of linear algebra, so for any of you who want a nice visual

[00:14:21 - 00:14:24]
understanding for matrices and what matrix vector multiplication means,

[00:14:24 - 00:14:28]
take a look at the series I did on linear algebra, especially chapter 3.

[00:14:29 - 00:14:33]
Back to our expression, instead of talking about adding the bias to each one of

[00:14:33 - 00:14:38]
these values independently, we represent it by organizing all those biases into

[00:14:38 - 00:14:42]
a vector, and adding the entire vector to the previous matrix vector product.

[00:14:43 - 00:14:46]
Then as a final step, I'll wrap a sigmoid around the outside here,

[00:14:46 - 00:14:50]
and what that's supposed to represent is that you're going to apply the

[00:14:50 - 00:14:54]
sigmoid function to each specific component of the resulting vector inside.

[00:14:55 - 00:15:00]
So once you write down this weight matrix and these vectors as their own symbols,

[00:15:00 - 00:15:05]
you can communicate the full transition of activations from one layer to the next in an

[00:15:05 - 00:15:10]
extremely tight and neat little expression, and this makes the relevant code both a lot

[00:15:10 - 00:15:14]
simpler and a lot faster, since many libraries optimize the heck out of matrix

[00:15:14 - 00:15:15]
multiplication.

[00:15:17 - 00:15:21]
Remember how earlier I said these neurons are simply things that hold numbers?

[00:15:22 - 00:15:27]
Well of course the specific numbers that they hold depends on the image you feed in,

[00:15:27 - 00:15:31]
so it's actually more accurate to think of each neuron as a function,

[00:15:31 - 00:15:36]
one that takes in the outputs of all the neurons in the previous layer and spits out a

[00:15:36 - 00:15:38]
number between 0 and 1.

[00:15:39 - 00:15:43]
Really the entire network is just a function, one that takes in

[00:15:43 - 00:15:47]
784 numbers as an input and spits out 10 numbers as an output.

[00:15:47 - 00:15:51]
It's an absurdly complicated function, one that involves 13,000 parameters

[00:15:51 - 00:15:55]
in the forms of these weights and biases that pick up on certain patterns,

[00:15:55 - 00:15:59]
and which involves iterating many matrix vector products and the sigmoid

[00:15:59 - 00:16:02]
squishification function, but it's just a function nonetheless.

[00:16:03 - 00:16:06]
And in a way it's kind of reassuring that it looks complicated.

[00:16:07 - 00:16:09]
I mean if it were any simpler, what hope would we have

[00:16:09 - 00:16:12]
that it could take on the challenge of recognizing digits?

[00:16:13 - 00:16:14]
And how does it take on that challenge?

[00:16:15 - 00:16:19]
How does this network learn the appropriate weights and biases just by looking at data?

[00:16:20 - 00:16:23]
Well that's what I'll show in the next video, and I'll also dig a little

[00:16:23 - 00:16:26]
more into what this particular network we're seeing is really doing.

[00:16:27 - 00:16:30]
Now is the point I suppose I should say subscribe to stay notified

[00:16:30 - 00:16:33]
about when that video or any new videos come out,

[00:16:33 - 00:16:37]
but realistically most of you don't actually receive notifications from YouTube, do you?

[00:16:38 - 00:16:41]
Maybe more honestly I should say subscribe so that the neural networks

[00:16:41 - 00:16:44]
that underlie YouTube's recommendation algorithm are primed to believe

[00:16:44 - 00:16:47]
that you want to see content from this channel get recommended to you.

[00:16:48 - 00:16:49]
Anyway, stay posted for more.

[00:16:50 - 00:16:53]
Thank you very much to everyone supporting these videos on Patreon.

[00:16:54 - 00:16:57]
I've been a little slow to progress in the probability series this summer,

[00:16:57 - 00:16:59]
but I'm jumping back into it after this project,

[00:16:59 - 00:17:01]
so patrons you can look out for updates there.

[00:17:03 - 00:17:07]
To close things off here I have with me Lisha Li who did her PhD work on the

[00:17:07 - 00:17:10]
theoretical side of deep learning and who currently works at a venture capital

[00:17:10 - 00:17:14]
firm called Amplify Partners who kindly provided some of the funding for this video.

[00:17:15 - 00:17:19]
So Lisha one thing I think we should quickly bring up is this sigmoid function.

[00:17:19 - 00:17:23]
As I understand it early networks use this to squish the relevant weighted

[00:17:23 - 00:17:26]
sum into that interval between zero and one, you know kind of motivated

[00:17:26 - 00:17:29]
by this biological analogy of neurons either being inactive or active.

[00:17:30 - 00:17:34]
Exactly. But relatively few modern networks actually use sigmoid anymore.

[00:17:34 - 00:17:35]
Yeah. It's kind of old school right?

[00:17:35 - 00:17:38]
Yeah or rather ReLU seems to be much easier to train.

[00:17:39 - 00:17:42]
And ReLU, ReLU stands for rectified linear unit?

[00:17:42 - 00:17:47]
Yes it's this kind of function where you're just taking a max of zero

[00:17:47 - 00:17:52]
and a where a is given by what you were explaining in the video and

[00:17:52 - 00:17:56]
what this was sort of motivated from I think was a partially by a

[00:17:56 - 00:18:01]
biological analogy with how neurons would either be activated or not.

[00:18:01 - 00:18:06]
And so if it passes a certain threshold it would be the identity function but if it did

[00:18:06 - 00:18:10]
not then it would just not be activated so it'd be zero so it's kind of a simplification.

[00:18:11 - 00:18:15]
Using sigmoids didn't help training or it was very difficult to

[00:18:15 - 00:18:20]
train at some point and people just tried ReLU and it happened

[00:18:20 - 00:18:24]
to work very well for these incredibly deep neural networks.
